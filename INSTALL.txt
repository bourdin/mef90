# Installing mef90 and vDef
vDef is build on top of mef90, a parallel unstructured finite element library and relies heavily on a patched version of petsc-3.3.
Building vDef requires:
  * C, C++, and fortran compilers
  * MPI with support for C++ and fortran90 (standard)
  * mercurial, python are required in order to build PETSc
  
Additionally, some utilities requires the following python modules:
  * numpy
  * matplotlib
  * JSON and argparse

In all that follows, it is assumed that the environment variable MEF90_DIR points to the root of the mef90 installation:
    [bourdin@head ~]$ echo $MEF90_DIR
    /home/bourdin/Development/mef90/mef90-sieve
    [bourdin@head ~]$ ls $MEF90_DIR
    bin       Makefile.include  mef90version.h  patches  tags        ThermoElasticity
    HeatXfer  m_DefMech         m_HeatXfer      python   TestMeshes  vDef
    Makefile  MEF90             objs            sanson   Tests
The actual content of the $MEF90_DIR folder may be somewhat different from the one shown here.

## Building PETSc-3.3:
* More instructions are provided at http://www.mcs.anl.gov/petsc/petsc-as/documentation/installation.html
* vDef is *NOT* compatible with petsc-3.4 and later. Port is under way.
* Download and extract the most recent petsc-3.3 tarball =.http://ftp.mcs.anl.gov/pub/petsc/release-snapshots/petsc-3.3-p7.tar.gz at the time of writing
* Set the PETSC_DIR environment variable to point to the extracted folder
    [bourdin@head petsc-3.3-p7]$ echo $PETSC_DIR
    /share/apps/petsc-3.3-p7
* set the PETSC_ARCH environment variable to a meaningful value. this value will be used by mef90 in order to allow out of tree build
    [bourdin@head petsc-3.3-p7]$ echo $PETSC_ARCH
    Linux-gcc4.4-mef90-O

* Patch petsc and its build system with the patches in $MEF90_DIR/patches:
    [bourdin@head petsc-3.3-p7]$ cd $PETSC_DIR; for d in $MEF90_DIR/patches/0*; do patch -p1 < $d; done
    [bourdin@head petsc-3.3-p7]$ cd $PETSC_DIR; for d in $MEF90_DIR/patches/f0*; do patch -p1 < $d; done  
    [bourdin@head BuildSystem]$ cd $PETSC_DIR/config/BuildSystem; for d in $MEF90_DIR/patches/b0*; do patch -p1 < $d; done

Note that the second command patches auto-generated fortran header files. If one builds petsc from a source code repository, they can be regenerated by running
    make allfortranstubs


    
### Configure petsc-3.3. 
  mef90 requires the following external packages: netcdf and exodusii, metis and parmetis, chaco, boost, triangle. 
  multi-threaded build require a recent version of cmake.
  petsc must be configured with fortran datatypes, sieve, and C++ as the C language.
  On a RHEL 6 linux system with the GNU compiler suite, the following configuration is a good starting point for a build with optimization
    ./configure                         \
      COPTFLAGS='-O3 -march=native'     \
      CXXOPTFLAGS='-O3 -march=native'   \
      FOPTFLAGS='-O3 -march=native'     \
      --download-boost=1                \
      --download-chaco=1                \
      --download-exodusii=1             \
      --download-metis=1                \
      --download-netcdf=1               \
      --download-parmetis=1             \
      --download-triangle=1             \
      --with-clanguage=C++              \
      --with-cmake=cmake                \
      --with-debugging=0                \
      --with-fortran-datatypes          \
      --with-mpi-dir=$MPI_HOME          \
      --with-shared-libraries=1         \
      --with-sieve
  
With intel 14.0 compilers, and MKL, I use
  ./configure                           \
      --COPTFLAGS=-O3                   \
      --CXXOPTFLAGS=-O3                 \
      --FOPTFLAGS=-O3                   \
      --LDFLAGS=-lstdc++                \
      --download-boost=1                \
      --download-chaco=1                \
      --download-exodusii=1             \
      --download-metis=1                \
      --download-netcdf=1               \
      --download-parmetis=1             \
      --download-triangle=1             \
      --with-blas-lapack-dir=$MKLROOT   \
      --with-clanguage=C++              \
      --with-cmake=cmake                \
      --with-debugging=0                \
      --with-fortran-datatypes          \
      --with-mpi-dir=$MPI_HOME          \
      --with-pic                        \
      --with-shared-libraries=1         \
      --with-sieve                      \
      --with-vendor-compilers=intel

In case of problems with X11, try --with-x=0


### Build petsc-3.3
  Follow the on-screen instruction to compile petsc from there (make PETSC_DIR=.... PETSC_ARCH=... )

Note that all fortran tests will fail as they are not compatible with the "--with-fortran-datatypes" configure option

### Optional:
  The exodus python bindings in $MEF90_DIR/exodus.py require that exodusii be compiled as a shared library, which is not the case by default. It is possible to replace the static libraries with dynamic ones:
  [bourdin@head petsc-3.3-p7]$ cd externalpackages/exodusii-5.22b/
  [bourdin@head exodusii-5.22b]$ gcc -shared -L $PETSC_DIR/$PETSC_ARCH/lib -lnetcdf -o $PETSC_DIR/$PETSC_ARCH/lib/libexodus.so cbind/src/*.o
  [bourdin@head exodusii-5.22b]$ gcc -shared -L $PETSC_DIR/$PETSC_ARCH/lib -lnetcdf -o $PETSC_DIR/$PETSC_ARCH/lib/libexoIIv2for.so forbind/src/*.o
  [bourdin@head exodusii-5.22b]$ rm $PETSC_DIR/$PETSC_ARCH/lib/libexo*.a

   
## Building vDef
From there, it should be as simple as 
    [bourdin@head ~]$ cd $MEF90_DIR; make
    
Note that the default setting is to link with shared libraries, and set their path using rpath (so that $LD_LIBRARY_PATH or equivalent does not have to be set).
This means that $PETSC_DIR/$PETSC_ARCH/lib needs to be readable from the compute nodes. If PETSc libraries are moved, use chrpath to change the search path after building vDef

## Testing:
  run make test in $MEF90_DIR/HeatXfer, $MEF90_DIR/ThermoElasticity, and $MEF90_DIR/vDef
  Differences in number of iterations, or round-off error are acceptable
  
  Note that make test will try to run mpi jobs directly. It may be necessary to run make tests in an interactive MPI job session.
  The MPI job launcher can be changed by setting the MPIEXEC environment variable.  

## More testing: